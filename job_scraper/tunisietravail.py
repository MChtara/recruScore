from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import json
import time
import random
import re

class DuplicateManager:
    """Gestionnaire de doublons simple"""
    
    def __init__(self):
        self.seen_urls = set()
        self.seen_jobs = set()
        self.duplicate_count = 0
        
    def is_duplicate(self, url, job_data):
        """V√©rifie si c'est un doublon (URL ou contenu)"""
        if url in self.seen_urls:
            self.duplicate_count += 1
            return True
        
        job_signature = (
            job_data.get("title", "").lower().strip(),
            job_data.get("company", "").lower().strip(),
            job_data.get("location", "").lower().strip()
        )
        
        if job_signature in self.seen_jobs:
            self.duplicate_count += 1
            return True
        
        self.seen_urls.add(url)
        self.seen_jobs.add(job_signature)
        return False

def clean_job_title(title):
    """
    Nettoie le titre pour extraire seulement le poste
    
    Exemples:
    - "Chahia Tunis recrute Technicien Sup√©rieur Informatique" 
      ‚Üí "Technicien Sup√©rieur Informatique"
    - "PRIMANET recrute Agent Commercial"
      ‚Üí "Agent Commercial"
    """
    if not title:
        return "Non pr√©cis√©"
    
    # Liste des mots-cl√©s de s√©paration
    separators = [
        'recrute',
        'recrutement de',
        'recherche',
        'cherche',
        'embauche',
        'offre d\'emploi',
        'offre'
    ]
    
    # Chercher le s√©parateur dans le titre
    title_lower = title.lower()
    for separator in separators:
        if separator in title_lower:
            # Trouver la position et extraire ce qui suit
            parts = title.split(separator, 1)
            if len(parts) == 2:
                cleaned_title = parts[1].strip()
                # Nettoyer les caract√®res sp√©ciaux en d√©but
                cleaned_title = cleaned_title.lstrip(':- ')
                if cleaned_title:
                    return cleaned_title
    
    # Si aucun s√©parateur trouv√©, retourner le titre tel quel
    return title.strip()

def extract_from_description(description):
    """
    Extrait les champs depuis la description
    
    Format attendu:
    Ville ‚Ä∫ Sfax ville
    Nom / Entreprise ‚Ä∫ PRIMANET TUNISIA
    Email ‚Ä∫ contact@email.com
    Adresse ‚Ä∫ Route El Ain sfax
    Tel / Fax ‚Ä∫ 27126506
    """
    
    fields = {
        "company": "Non pr√©cis√©",
        "location": "Non pr√©cis√©",
        "email": "Non pr√©cis√©",
        "address": "Non pr√©cis√©",
        "tel": "Non pr√©cis√©"
    }
    
    if not description:
        return fields
    
    # Patterns pour les champs structur√©s
    patterns = {
        "location": [
            r"Ville\s*[‚Ä∫:]\s*([^\n‚Ä∫]+)",
            r"Lieu\s*[‚Ä∫:]\s*([^\n‚Ä∫]+)"
        ],
        "company": [
            r"(?:Nom\s*/\s*)?Entreprise\s*[‚Ä∫:]\s*([^\n‚Ä∫]+)",
            r"Soci√©t√©\s*[‚Ä∫:]\s*([^\n‚Ä∫]+)"
        ],
        "email": [
            r"Email\s*[‚Ä∫:]\s*([^\n‚Ä∫\s]+@[^\n‚Ä∫\s]+)",
            r"E-mail\s*[‚Ä∫:]\s*([^\n‚Ä∫\s]+@[^\n‚Ä∫\s]+)"
        ],
        "address": [
            r"Adresse\s*[‚Ä∫:]\s*([^\n‚Ä∫]+)"
        ],
        "tel": [
            r"Tel\s*(?:/\s*Fax)?\s*[‚Ä∫:]\s*([^\n‚Ä∫]+)",
            r"T√©l√©phone\s*[‚Ä∫:]\s*([^\n‚Ä∫]+)"
        ]
    }
    
    # Extraire les champs
    for field, field_patterns in patterns.items():
        for pattern in field_patterns:
            match = re.search(pattern, description, re.IGNORECASE | re.MULTILINE)
            if match:
                value = match.group(1).strip()
                if value and value != '‚Ä∫' and len(value) > 1:
                    fields[field] = value
                    break
    
    return fields

def setup_driver():
    """Configuration Selenium"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
    
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        return driver
    except Exception as e:
        print(f"‚ùå Erreur navigateur: {e}")
        return None

def perform_search(driver, ville, secteur):
    """Effectue une recherche cibl√©e sur le site"""
    try:
        print(f"üîç Acc√®s √† la page de recherche de tunisietravail.net")
        driver.get("https://www.tunisietravail.net/")
        time.sleep(3)
        
        search_selectors = [
            "input[name='search']",
            "input[type='search']", 
            "input[placeholder*='recherch']",
            "input[placeholder*='emploi']",
            "#search",
            ".search-input",
            "input.form-control"
        ]
        
        search_box = None
        for selector in search_selectors:
            try:
                search_box = driver.find_element(By.CSS_SELECTOR, selector)
                if search_box.is_displayed():
                    break
            except:
                continue
        
        if not search_box:
            print("‚ö†Ô∏è Champ de recherche non trouv√©, tentative avec URL de recherche directe")
            search_query = f"{ville} {secteur}"
            search_url = f"https://www.tunisietravail.net/?s={search_query.replace(' ', '+')}"
            driver.get(search_url)
            time.sleep(3)
            return True
        
        search_query = f"{ville} {secteur}"
        print(f"üîé Recherche de: '{search_query}'")
        
        search_box.clear()
        search_box.send_keys(search_query)
        search_box.send_keys(Keys.RETURN)
        
        time.sleep(5)
        print("‚úÖ Recherche effectu√©e avec succ√®s")
        return True
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la recherche: {e}")
        return False

def scrape_job_details(driver, job_link):
    """Extraction des d√©tails d'une offre - VERSION AM√âLIOR√âE"""
    try:
        driver.get(job_link)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "h1"))
        )
        time.sleep(random.uniform(2, 4))
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # 1. Extraire et nettoyer le titre
        title = "Non pr√©cis√©"
        title_tag = soup.find("h1")
        if title_tag:
            raw_title = title_tag.get_text().strip()
            title = clean_job_title(raw_title)  # Nettoyer le titre
        
        # 2. Extraire la description compl√®te
        description = "Non pr√©cis√©"
        description_selectors = [
            ".PostContent",
            ".entry-content",
            ".job-description", 
            ".content",
            "article .content"
        ]
        
        for selector in description_selectors:
            description_tag = soup.select_one(selector)
            if description_tag:
                description = description_tag.get_text().strip()
                break
        
        # 3. Extraire les champs depuis la description
        extracted_fields = extract_from_description(description)
        
        # 4. Cr√©er l'objet job avec les vraies donn√©es
        job_data = {
            "title": title,
            "company": extracted_fields["company"],
            "location": extracted_fields["location"],
            "email": extracted_fields["email"],
            "address": extracted_fields["address"],
            "tel": extracted_fields["tel"],
            "job_url": job_link,
            "description": description,
            "source": "Tunisie Travail",
            "date": "Non pr√©cis√©",
            "job_type": "N/A",
            "salary": "Non pr√©cis√©",
            "contrat": "N/A"
        }
        
        return job_data
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur extraction: {e}")
        return None

def scrape_search_results(driver, ville, secteur, max_pages=10):
    """Scrape les r√©sultats de recherche"""
    
    duplicate_manager = DuplicateManager()
    matching_jobs = []
    
    try:
        for page in range(1, max_pages + 1):
            print(f"\nüìÑ Page {page}/{max_pages} des r√©sultats de recherche")
            
            if page > 1:
                current_url = driver.current_url
                if "page/" in current_url:
                    base_url = current_url.split("page/")[0]
                    driver.get(f"{base_url}page/{page}/")
                else:
                    separator = "&" if "?" in current_url else "?"
                    driver.get(f"{current_url}{separator}paged={page}")
                
                time.sleep(3)
            
            soup = BeautifulSoup(driver.page_source, "html.parser")
            
            job_selectors = [
                ".Post h2 a[href]",
                ".Post a[href*='emploi']",
                ".Post a[href*='recrute']",
                "article.Post a[href]",
                ".entry-title a[href]",
                ".job-title a[href]",
                ".Post .entry-header a[href]"
            ]
            
            job_links = []
            for selector in job_selectors:
                links = soup.select(selector)
                if links:
                    filtered_links = []
                    for link in links:
                        href = link.get('href', '')
                        link_text = link.get_text().lower()
                        
                        exclude_keywords = [
                            'facebook', 'twitter', 'linkedin', 'instagram',
                            's\'identifier', 'connexion', 'inscription', 'contact',
                            'javascript', 'cookies', 'confidentialit√©', 'mentions',
                            'accueil', '√† propos', 'services', 'blog'
                        ]
                        
                        include_keywords = [
                            'recrute', 'emploi', 'offre', 'poste', 'candidat',
                            'technicien', 'ing√©nieur', 'commercial', 'assistant',
                            'responsable', 'chef', 'directeur', 'agent', 'stage'
                        ]
                        
                        is_excluded = any(keyword in link_text for keyword in exclude_keywords)
                        is_job_related = any(keyword in link_text for keyword in include_keywords)
                        has_valid_href = href and not href.startswith('#') and len(href) > 10
                        
                        if not is_excluded and is_job_related and has_valid_href:
                            filtered_links.append(href)
                    
                    if filtered_links:
                        job_links = filtered_links
                        print(f"‚úÖ Trouv√© {len(job_links)} liens d'offres")
                        break
            
            if len(job_links) > 50:
                print(f"‚ö†Ô∏è Limitation √† 50 premiers liens")
                job_links = job_links[:50]
            
            if not job_links:
                print("‚ùå Aucun lien d'offre valide trouv√©")
                break
            
            page_matches = 0
            
            for i, job_link in enumerate(job_links, 1):
                try:
                    if job_link.startswith('/'):
                        job_link = "https://www.tunisietravail.net" + job_link
                    elif not job_link.startswith('http'):
                        job_link = "https://www.tunisietravail.net/" + job_link
                    
                    print(f"  üìã Analyse offre {i}/{len(job_links)}")
                    
                    job_data = scrape_job_details(driver, job_link)
                    if not job_data:
                        continue
                    
                    if duplicate_manager.is_duplicate(job_link, job_data):
                        print(f"    üîÑ Doublon ignor√©")
                        continue
                    
                    matching_jobs.append(job_data)
                    page_matches += 1
                    print(f"    ‚úÖ {job_data['title'][:40]}... - {job_data['company']}")
                    
                    time.sleep(random.uniform(2, 4))
                    
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Erreur: {e}")
                    continue
            
            print(f"üìä Page {page}: {page_matches} nouvelles offres")
            print(f"üìä Total: {len(matching_jobs)} offres")
            
            time.sleep(random.uniform(3, 6))
    
    except Exception as e:
        print(f"‚ùå Erreur scraping: {e}")
    
    return matching_jobs, duplicate_manager

def scrape_tunisie_travail(ville, secteur, max_pages=10):
    """Scrape les offres d'emploi avec recherche cibl√©e"""
    
    print(f"Recherche: {ville.upper()} + {secteur.upper()}")
    print(f"Pages: {max_pages}")
    
    driver = setup_driver()
    if not driver:
        return []
    
    try:
        if not perform_search(driver, ville, secteur):
            print("Erreur: Impossible d'effectuer la recherche")
            return []
        
        matching_jobs, duplicate_manager = scrape_search_results(driver, ville, secteur, max_pages)
        
    except Exception as e:
        print(f"Erreur: {e}")
        matching_jobs = []
    finally:
        driver.quit()
    
    print(f"\nTermin√©: {len(matching_jobs)} offres trouv√©es")
    
    return matching_jobs  # Retourne directement la liste

def save_to_json(data, filename):
    """Sauvegarde en JSON"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"üíæ Sauvegard√© dans: {filename}")
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde: {e}")

if __name__ == "__main__":
    
    print("="*60)
    print("RECHERCHE: TUNIS + INFORMATIQUE")
    print("="*60)
    
    # R√©cup√®re directement la liste des offres
    offres = scrape_tunisie_travail(
        ville="tunis",
        secteur="informatique",
        max_pages=1
    )
    
    # Sauvegarder directement la liste
    save_to_json(offres, "tunis_travail.json")
    
    print(f"\nTotal: {len(offres)} offres")