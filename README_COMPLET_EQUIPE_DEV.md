# ğŸ¯ JobMatch Pro - Documentation Technique ComplÃ¨te

**Plateforme de Matching d'Emplois avec ATS Intelligent**

Version: 2.0 | Date: 2025-11-13 | Ã‰quipe: DÃ©veloppement & IntÃ©gration

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'Ensemble](#-vue-densemble)
2. [Architecture du Projet](#-architecture-du-projet)
3. [Structure des Fichiers](#-structure-des-fichiers---utilitÃ©-dÃ©taillÃ©e)
4. [Technologies UtilisÃ©es](#-technologies-utilisÃ©es)
5. [Installation](#-installation)
6. [Configuration](#-configuration)
7. [Workflows Principaux](#-workflows-principaux)
8. [API et Endpoints](#-api-et-endpoints)
9. [Base de DonnÃ©es](#-base-de-donnÃ©es)
10. [DÃ©ploiement](#-dÃ©ploiement)
11. [Maintenance](#-maintenance)

---

## ğŸ¯ Vue d'Ensemble

### FonctionnalitÃ©s Principales

**JobMatch Pro** est une plateforme complÃ¨te qui combine:

1. **Scraping Multi-Sources** d'offres d'emploi (Google Jobs, LinkedIn, PÃ´le Emploi, Tunisie Travail)
2. **Analyse ATS** (Applicant Tracking System) avec IA pour matcher CV â†” Offres
3. **Recommandation de Cours Coursera** via recherche vectorielle (ChromaDB)
4. **GÃ©nÃ©ration de Tests Techniques** personnalisÃ©s basÃ©s sur le CV
5. **VÃ©rification de Documents** (certificats, attestations) via Vision AI
6. **Dashboard Statistiques** temps rÃ©el avec visualisations

### Points Forts

- âœ… **16,416 cours Coursera** prÃ©-scrapÃ©s avec embeddings vectoriels
- âœ… **Recherche 10x plus rapide** grÃ¢ce Ã  ChromaDB (< 1 seconde)
- âœ… **Analyse automatique** du CV dÃ¨s l'upload
- âœ… **Multi-sources** de scraping avec dÃ©duplication intelligente
- âœ… **Vision AI** pour vÃ©rification automatique des preuves

---

## ğŸ—ï¸ Architecture du Projet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FRONTEND (Bootstrap 5)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Templates Jinja2 + Chart.js + JavaScript Vanilla        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND (Flask 3.0)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Routes REST API + Session Management + File Uploads     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   JOB SCRAPERS      â”‚   ATS SCORER (AI)    â”‚  COURSE RECOMMENDER â”‚
â”‚                     â”‚                      â”‚                     â”‚
â”‚ â€¢ Google Jobs API   â”‚ â€¢ Groq AI (Llama)    â”‚ â€¢ ChromaDB          â”‚
â”‚ â€¢ LinkedIn (Free)   â”‚ â€¢ CV Parsing         â”‚ â€¢ Sentence-BERT     â”‚
â”‚ â€¢ PÃ´le Emploi API   â”‚ â€¢ Skill Extraction   â”‚ â€¢ Coursera Scraper  â”‚
â”‚ â€¢ Tunisie Travail   â”‚ â€¢ Vision AI (VLM)    â”‚ â€¢ Quiz Generator    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STOCKAGE DE DONNÃ‰ES                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  jobs.db    â”‚ coursera_    â”‚  chroma_db/ â”‚  user_cvs/      â”‚â”‚
â”‚  â”‚  (SQLite)   â”‚  fast.db     â”‚  (Vector    â”‚  user_proofs/   â”‚â”‚
â”‚  â”‚             â”‚  (16,416     â”‚   Store)    â”‚  (Files)        â”‚â”‚
â”‚  â”‚  15 MB      â”‚   cours)     â”‚             â”‚                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Structure des Fichiers - UtilitÃ© DÃ©taillÃ©e

### ğŸ“‚ Racine du Projet

```
CV/
â”œâ”€â”€ ğŸ“„ app.py                          (1,971 lignes)
â”‚   â””â”€ Application Flask principale
â”‚      â€¢ Routes HTTP (/, /job/<id>, /stats, /api/*, etc.)
â”‚      â€¢ Gestion de sessions utilisateur
â”‚      â€¢ Upload/Analyse de CV automatique
â”‚      â€¢ IntÃ©gration avec tous les modules
â”‚      â€¢ Administration ChromaDB & Scraping
â”‚      CRITIQUE: CÅ“ur de l'application
â”‚
â”œâ”€â”€ ğŸ“„ ats_scorer.py                   (134 KB)
â”‚   â””â”€ Module d'analyse ATS avec IA
â”‚      â€¢ analyser_cv_avec_offre(): Matching CV â†” Job (score 0-100%)
â”‚      â€¢ extraire_competences_techniques(): Extraction skills du CV
â”‚      â€¢ recommander_cours(): Recherche vectorielle ChromaDB
â”‚      â€¢ generer_quiz_coursera(): GÃ©nÃ©ration tests personnalisÃ©s
â”‚      â€¢ verifier_document_vlm(): Vision AI pour preuves
â”‚      â€¢ IntÃ©gration Groq AI (Llama 3.1 70B)
â”‚      CRITIQUE: Intelligence artificielle du systÃ¨me
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt                (750 bytes)
â”‚   â””â”€ DÃ©pendances Python
â”‚      â€¢ Flask 3.0.0, pandas, requests
â”‚      â€¢ PyPDF2, python-docx, Pillow
â”‚      â€¢ beautifulsoup4, selenium
â”‚      â€¢ sentence-transformers, chromadb
â”‚      ESSENTIEL: Installation des librairies
â”‚
â”œâ”€â”€ ğŸ“„ .env                            (70 bytes)
â”‚   â””â”€ Variables d'environnement (SENSIBLE)
â”‚      â€¢ ATS_API_KEY=gsk_xxx (Groq AI)
â”‚      â€¢ SECRET_KEY=xxx (Flask session)
â”‚      âš ï¸  NE JAMAIS COMMITTER CE FICHIER
â”‚
â”œâ”€â”€ ğŸ“„ .gitignore                      (158 bytes)
â”‚   â””â”€ Fichiers exclus du versioning
â”‚      â€¢ __pycache__/, .env, user_cvs/
â”‚      â€¢ user_proofs/, uploads/, nul
â”‚
â”œâ”€â”€ ğŸ“„ START_APP.bat                   (211 bytes)
â”‚   â””â”€ Script de dÃ©marrage Windows
â”‚      â€¢ Lance "python app.py" avec interface jolie
â”‚      UTILE: Pour utilisateurs non-techniques
â”‚
â”œâ”€â”€ ğŸ“„ jobs.db                         (15.6 MB)
â”‚   â””â”€ Base de donnÃ©es SQLite des offres d'emploi
â”‚      â€¢ Table "jobs" avec ~5000+ offres scrapÃ©es
â”‚      â€¢ Colonnes: title, company, location, description, etc.
â”‚      â€¢ Index optimisÃ©s pour recherche rapide
â”‚      CRITIQUE: Stockage des offres d'emploi
â”‚
â””â”€â”€ ğŸ“„ README.md                       (15.5 KB)
    â””â”€ Documentation utilisateur existante
       â€¢ Guide d'installation et utilisation
       â€¢ Exemples de configuration
       Ã€ REMPLACER ou COMPLÃ‰TER par ce README
```

---

### ğŸ“‚ job_scraper/ - Module de Scraping d'Offres

```
job_scraper/
â”œâ”€â”€ ğŸ“„ db_manager.py
â”‚   â””â”€ Gestionnaire de base de donnÃ©es jobs.db
â”‚      â€¢ Classe JobDatabase avec mÃ©thodes CRUD
â”‚      â€¢ bulk_insert_jobs(): Insertion en masse avec dÃ©duplication
â”‚      â€¢ search_jobs(): Recherche avec filtres multiples
â”‚      â€¢ get_statistics(): Statistiques par source/entreprise
â”‚      â€¢ export_to_csv(): Export des donnÃ©es
â”‚      CRITIQUE: Interface unique pour tous les scrapers
â”‚
â”œâ”€â”€ ğŸ“„ google_jobs.py
â”‚   â””â”€ Scraper Google Jobs via API officielle
â”‚      â€¢ scrape_google_jobs(query, api_key, max_results)
â”‚      â€¢ Support multi-pays/langues
â”‚      â€¢ Extraction complÃ¨te (titre, entreprise, description, URL)
â”‚      AVANTAGE: API officielle, donnÃ©es structurÃ©es
â”‚
â”œâ”€â”€ ğŸ“„ Linkedin.py
â”‚   â””â”€ Scraper LinkedIn (mÃ©thode libre sans API payante)
â”‚      â€¢ scrape_linkedin_jobs_free(keywords, location, pages)
â”‚      â€¢ Parsing HTML avec BeautifulSoup
â”‚      â€¢ Gestion du rate limiting (dÃ©lais entre requÃªtes)
â”‚      NOTE: Peut nÃ©cessiter ajustements si LinkedIn change HTML
â”‚
â”œâ”€â”€ ğŸ“„ france_travail.py
â”‚   â””â”€ Scraper API PÃ´le Emploi (France Travail)
â”‚      â€¢ scrape_france_travail(client_id, secret, days)
â”‚      â€¢ Authentification OAuth2
â”‚      â€¢ Filtres: localisation, type de contrat, secteur
â”‚      AVANTAGE: API officielle avec donnÃ©es certifiÃ©es
â”‚
â”œâ”€â”€ ğŸ“„ tunisietravail.py
â”‚   â””â”€ Scraper site Tunisie Travail
â”‚      â€¢ scrape_tunisie_travail(ville, secteur, max_pages)
â”‚      â€¢ Parsing HTML spÃ©cifique au site tunisien
â”‚      â€¢ Extraction: titre, entreprise, date, URL
â”‚
â””â”€â”€ ğŸ“„ config.json
    â””â”€ Configuration des scrapers
       â€¢ Activation/dÃ©sactivation par source
       â€¢ ParamÃ¨tres: queries, keywords, API keys, limites
       FORMAT: JSON modifiable via interface admin
```

**Workflow du Scraping:**
```
1. Admin lance scraping via /admin/scraping
2. Chaque scraper retourne liste de jobs (format standardisÃ©)
3. db_manager.bulk_insert_jobs() dÃ©duplique et insÃ¨re
4. jobs.db se met Ã  jour en temps rÃ©el
5. Interface utilisateur affiche instantanÃ©ment les nouvelles offres
```

---

### ğŸ“‚ course_scraper/ - Recommandation de Cours Coursera

```
course_scraper/
â”œâ”€â”€ ğŸ“„ coursera_fast.db                (31 MB)
â”‚   â””â”€ Base SQLite avec 16,416 cours Coursera
â”‚      â€¢ Table "courses": id, title, description, difficulty, url, partner
â”‚      â€¢ Index sur: course_id, difficulty, title
â”‚      â€¢ DonnÃ©es prÃ©-scrapÃ©es (gain de temps Ã©norme)
â”‚      CRITIQUE: Base de connaissance pour recommandations
â”‚
â”œâ”€â”€ ğŸ“„ course_embedding_store.py       (16 KB)
â”‚   â””â”€ Gestionnaire ChromaDB (Vector Store)
â”‚      â€¢ Classe CourseEmbeddingStore
â”‚      â€¢ search_similar_courses(query, n_results, where)
â”‚        â†’ Recherche vectorielle < 1 seconde
â”‚      â€¢ add_courses_batch(): Ajout en masse avec embeddings
â”‚      â€¢ sync_from_sqlite(): Synchronisation SQLite â†’ ChromaDB
â”‚      â€¢ Sentence-BERT (all-MiniLM-L6-v2) 384 dimensions
â”‚      CRITIQUE: CÅ“ur de l'optimisation 10x
â”‚
â”œâ”€â”€ ğŸ“‚ chroma_db/                      (dossier persistant)
â”‚   â””â”€ chroma.sqlite3 + index HNSW
â”‚      â€¢ 16,416 embeddings prÃ©-calculÃ©s
â”‚      â€¢ Algorithme: Hierarchical Navigable Small World
â”‚      â€¢ MÃ©trique: SimilaritÃ© cosinus
â”‚      âš ï¸  Ã€ gÃ©nÃ©rer au premier dÃ©ploiement
â”‚
â”œâ”€â”€ ğŸ“„ coursera_scraper_simple.py      (11 KB)
â”‚   â””â”€ Scraper Coursera rapide
â”‚      â€¢ scrape_courses(max_courses, sync_chromadb=True)
â”‚      â€¢ Extraction via API interne Coursera
â”‚      â€¢ Synchronisation automatique ChromaDB
â”‚      USAGE: Mise Ã  jour hebdomadaire (weekly_update.py)
â”‚
â”œâ”€â”€ ğŸ“„ coursera_scraper_utils.py       (25 KB)
â”‚   â””â”€ Utilitaires scraping dÃ©taillÃ© des cours
â”‚      â€¢ scrape_what_you_learn(course_url)
â”‚        â†’ Extrait "What you'll learn" (objectifs)
â”‚      â€¢ scrape_course_modules_details(course_url)
â”‚        â†’ Scrape modules complets (nom, topics, durÃ©e)
â”‚      â€¢ format_modules_for_quiz_context()
â”‚        â†’ Formate pour gÃ©nÃ©ration de quiz IA
â”‚      USAGE: GÃ©nÃ©ration de tests techniques prÃ©cis
â”‚
â”œâ”€â”€ ğŸ“„ weekly_update.py                (12 KB)
â”‚   â””â”€ Script de mise Ã  jour hebdomadaire
â”‚      â€¢ DÃ©tecte nouveaux cours Coursera (scraping intelligent)
â”‚      â€¢ ArrÃªt auto: 3 batchs consÃ©cutifs sans nouveaux cours
â”‚      â€¢ Synchronise ChromaDB automatiquement
â”‚      â€¢ GÃ©nÃ¨re rapport (update_log.txt)
â”‚      CRON: Ã€ exÃ©cuter 1x par semaine (dimanche 2h AM)
â”‚
â”œâ”€â”€ ğŸ“„ migrate_embeddings_chromadb.py  (4 KB)
â”‚   â””â”€ Script de migration initiale ChromaDB
â”‚      â€¢ Charge tous les cours depuis coursera_fast.db
â”‚      â€¢ GÃ©nÃ¨re embeddings Sentence-BERT
â”‚      â€¢ Stocke dans chroma_db/
â”‚      âš ï¸  EXÃ‰CUTER UNE SEULE FOIS au premier dÃ©ploiement
â”‚      Temps: ~5-10 minutes pour 16,416 cours
â”‚
â”œâ”€â”€ ğŸ“„ benchmark_chromadb.py           (7 KB)
â”‚   â””â”€ Benchmark ChromaDB vs mÃ©thode classique
â”‚      â€¢ Compare temps de recherche
â”‚      â€¢ VÃ©rifie qualitÃ© des rÃ©sultats
â”‚      USAGE: Tests de performance (optionnel)
â”‚
â”œâ”€â”€ ğŸ“„ test_chromadb_setup.py          (3 KB)
â”‚   â””â”€ Test d'installation ChromaDB
â”‚      â€¢ VÃ©rifie connexion, compte embeddings
â”‚      â€¢ Test recherche simple
â”‚      USAGE: Validation post-installation
â”‚
â””â”€â”€ ğŸ“„ Documentation (3 fichiers .md)
    â”œâ”€ README_CHROMADB.md              (7.8 KB)
    â”‚  â””â”€ Documentation complÃ¨te ChromaDB
    â”œâ”€ QUICK_START.md                  (3 KB)
    â”‚  â””â”€ Guide dÃ©marrage rapide
    â””â”€ INSTALLATION_CHROMADB.md        (5.2 KB)
       â””â”€ Guide d'installation dÃ©taillÃ©
```

**Workflow Recommandation de Cours:**
```
1. Utilisateur uploade CV â†’ ATS extrait compÃ©tences
2. Pour chaque compÃ©tence manquante:
   a. ats_scorer.recommander_cours(competence, niveau, contexte)
   b. CourseEmbeddingStore.search_similar_courses() [ChromaDB]
   c. Scoring hybride: 60% sÃ©mantique + 40% correspondance nom
   d. Filtrage par niveau avec fallback intelligent
3. RÃ©sultats retournÃ©s en < 1 seconde (vs 5-10s avant)
4. Top 3 cours par compÃ©tence affichÃ©s Ã  l'utilisateur
```

---

### ğŸ“‚ templates/ - Templates HTML Jinja2

```
templates/
â”œâ”€â”€ ğŸ“„ base.html
â”‚   â””â”€ Template de base (extends par tous les autres)
â”‚      â€¢ Header avec navigation
â”‚      â€¢ Footer
â”‚      â€¢ Inclusion Bootstrap 5, Chart.js
â”‚      â€¢ Flash messages
â”‚
â”œâ”€â”€ ğŸ“„ index.html
â”‚   â””â”€ Page d'accueil / Recherche d'emplois
â”‚      â€¢ Barre de recherche multi-critÃ¨res
â”‚      â€¢ Filtres avancÃ©s (localisation, entreprise, date, etc.)
â”‚      â€¢ Liste des offres avec pagination
â”‚      â€¢ Statistiques en temps rÃ©el
â”‚
â”œâ”€â”€ ğŸ“„ job_detail.html
â”‚   â””â”€ DÃ©tail d'une offre + Analyse ATS automatique
â”‚      â€¢ Affichage offre complÃ¨te
â”‚      â€¢ SI CV uploadÃ©: Analyse automatique dÃ¨s chargement
â”‚      â€¢ Score de compatibilitÃ© (0-100%)
â”‚      â€¢ Recommandations de cours pour skills manquantes
â”‚      â€¢ Bouton "Postuler" vers l'URL originale
â”‚
â”œâ”€â”€ ğŸ“„ stats.html
â”‚   â””â”€ Dashboard statistiques avec Chart.js
â”‚      â€¢ Graphiques: Offres par ville, par entreprise, par type
â”‚      â€¢ Tendances temporelles
â”‚      â€¢ Distribution des contrats
â”‚      â€¢ Sources de scraping
â”‚
â”œâ”€â”€ ğŸ“„ upload_cv_page.html
â”‚   â””â”€ Page de gestion du CV utilisateur
â”‚      â€¢ Upload CV (PDF, DOCX, TXT)
â”‚      â€¢ Preview du nom de fichier
â”‚      â€¢ Bouton suppression
â”‚      â€¢ Liens vers: VÃ©rification preuves, Tests techniques
â”‚
â”œâ”€â”€ ğŸ“„ technical_tests.html
â”‚   â””â”€ GÃ©nÃ©ration et passage de tests techniques
â”‚      â€¢ Liste des compÃ©tences extraites du CV
â”‚      â€¢ GÃ©nÃ©ration de quiz personnalisÃ© (10 QCM)
â”‚      â€¢ Passage du test avec timer
â”‚      â€¢ Ã‰valuation automatique + feedback
â”‚      â€¢ Mise Ã  jour du niveau de compÃ©tence
â”‚
â”œâ”€â”€ ğŸ“„ verify_credentials.html
â”‚   â””â”€ VÃ©rification des certificats/attestations
â”‚      â€¢ Liste des certificats extraits du CV
â”‚      â€¢ Upload de preuves (PDF, images)
â”‚      â€¢ VÃ©rification automatique via Vision AI
â”‚      â€¢ Statut: ConfirmÃ©, Partiellement confirmÃ©, Non confirmÃ©
â”‚
â”œâ”€â”€ ğŸ“„ admin_scraping.html
â”‚   â””â”€ Dashboard admin pour scraping
â”‚      â€¢ Configuration des scrapers (activation, params)
â”‚      â€¢ Lancement manuel par source
â”‚      â€¢ Suivi temps rÃ©el (progression, logs)
â”‚      â€¢ Statistiques: Total jobs, par source, doublons
â”‚      â€¢ Export CSV
â”‚
â”œâ”€â”€ ğŸ“„ admin_chromadb.html
â”‚   â””â”€ Dashboard admin ChromaDB
â”‚      â€¢ Statut: Nombre d'embeddings vs cours en DB
â”‚      â€¢ Migration initiale (bouton "Migrer")
â”‚      â€¢ Progression en temps rÃ©el
â”‚      â€¢ VÃ©rification santÃ© du vector store
â”‚
â”œâ”€â”€ ğŸ“„ cv_analysis.html
â”‚   â””â”€ Affichage rÃ©sultat analyse ATS dÃ©taillÃ©e
â”‚      â€¢ Score global, breakdown par catÃ©gorie
â”‚      â€¢ Skills trouvÃ©es vs manquantes
â”‚      â€¢ Recommandations personnalisÃ©es
â”‚
â”œâ”€â”€ ğŸ“„ cv_upload.html
â”‚   â””â”€ Formulaire upload CV (route /analyze-cv/<job_id>)
â”‚      â€¢ Upload pour une offre spÃ©cifique
â”‚      â€¢ Analyse immÃ©diate aprÃ¨s upload
â”‚
â””â”€â”€ ğŸ“„ test_jobs.html
    â””â”€ Page de test pour dÃ©veloppement
       â€¢ Debug: Affichage brut des offres
```

---

### ğŸ“‚ static/ - Fichiers Statiques

```
static/
â”œâ”€â”€ css/
â”‚   â”œâ”€â”€ ğŸ“„ main.css
â”‚   â”‚   â””â”€ Styles principaux de l'application
â”‚   â”‚      â€¢ Layout, colors, typography
â”‚   â”‚      â€¢ Composants personnalisÃ©s (cards, badges)
â”‚   â”‚      â€¢ Responsive design
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“„ style.css
â”‚       â””â”€ Styles additionnels/overrides
â”‚
â””â”€â”€ js/
    â”œâ”€â”€ ğŸ“„ main.js
    â”‚   â””â”€ JavaScript principal
    â”‚      â€¢ Gestion des filtres de recherche
    â”‚      â€¢ Pagination AJAX
    â”‚      â€¢ Upload de fichiers avec preview
    â”‚      â€¢ Flash messages animÃ©s
    â”‚
    â””â”€â”€ ğŸ“„ app.js
        â””â”€ JavaScript applicatif
           â€¢ IntÃ©gration Chart.js pour graphiques
           â€¢ Appels API AJAX (/api/search, /api/recommend-courses)
           â€¢ Gestion des tests techniques (timer, soumission)
           â€¢ VÃ©rification de documents (upload + VLM)
```

---

### ğŸ“‚ Dossiers de DonnÃ©es Utilisateur

```
user_cvs/
â””â”€ CVs uploadÃ©s par les utilisateurs
   â€¢ Noms: <session_id>.pdf/.docx
   â€¢ Stockage persistant (non versionnÃ© Git)
   âš ï¸  Ã€ sauvegarder rÃ©guliÃ¨rement

user_proofs/
â””â”€ Preuves (certificats, attestations) uploadÃ©es
   â€¢ Structure: <session_id>/<categorie>/<item_index>_filename.pdf
   â€¢ UtilisÃ© pour vÃ©rification VLM
   âš ï¸  Ã€ sauvegarder rÃ©guliÃ¨rement

uploads/
â””â”€ Fichiers temporaires (intermÃ©diaires)
   â€¢ Nettoyage automatique recommandÃ©
```

---

### ğŸ—‘ï¸ Fichiers Ã€ SUPPRIMER (Non Essentiels)

```
âŒ nul                                  (56 bytes)
   â†’ Fichier vide/erreur Windows

âŒ updates.txt                          (5.6 KB)
   â†’ Notes de dÃ©veloppement (dÃ©jÃ  dans .gitignore)

âŒ check_db_courses.py                  (663 bytes)
   â†’ Script de test (usage unique)

âŒ test_scraper_topics.py               (1 KB)
   â†’ Script de test (usage unique)

âŒ tests_indisponibles.log              (2 bytes)
   â†’ Log vide

âŒ templates/job_detail.html.backup
   â†’ Backup temporaire

âŒ EXPLICATION_SCRAPING_GENERATION_TESTS.md (29 KB)
   â†’ Documentation technique (redondante avec ce README)

âŒ cours_recommandation_semantic.py     (7.5 KB)
   â†’ Ancien script OBSOLÃˆTE (remplacÃ© par ChromaDB)

âŒ course_scraper/migrate_embeddings.py (10 KB)
   â†’ Ancienne version migration (remplacÃ©e par migrate_embeddings_chromadb.py)
```

**Action recommandÃ©e:**
```bash
# Supprimer les fichiers non essentiels
rm nul updates.txt check_db_courses.py test_scraper_topics.py tests_indisponibles.log
rm templates/job_detail.html.backup cours_recommandation_semantic.py
rm EXPLICATION_SCRAPING_GENERATION_TESTS.md
rm course_scraper/migrate_embeddings.py
```

---

## ğŸ› ï¸ Technologies UtilisÃ©es

### Backend

| Technologie | Version | RÃ´le |
|-------------|---------|------|
| **Python** | 3.8+ | Langage principal |
| **Flask** | 3.0.0 | Framework web |
| **Pandas** | 2.1.4 | Manipulation de donnÃ©es |
| **SQLite** | 3.x | Base de donnÃ©es |
| **Sentence-Transformers** | Latest | Embeddings sÃ©mantiques |
| **ChromaDB** | Latest | Vector database |
| **Groq AI** | API | LLM (Llama 3.1 70B) |
| **BeautifulSoup4** | 4.12.2 | Web scraping HTML |
| **Selenium** | 4.16.0 | Scraping dynamique (optionnel) |
| **PyPDF2** | 3.0.1 | Extraction texte PDF |
| **python-docx** | 1.1.0 | Extraction texte Word |
| **Pillow** | 10.1.0 | Traitement d'images |

### Frontend

| Technologie | CDN | RÃ´le |
|-------------|-----|------|
| **Bootstrap** | 5.3 | Framework CSS |
| **Chart.js** | 4.4.0 | Graphiques interactifs |
| **JavaScript Vanilla** | ES6 | InteractivitÃ© |
| **Jinja2** | (Flask) | Templating |

### APIs Externes

| Service | Usage | Requis? |
|---------|-------|---------|
| **Groq AI** | Analyse ATS, extraction skills, quiz | âœ… OUI |
| **Google Jobs API** | Scraping offres Google | âŒ Optionnel |
| **France Travail API** | Scraping PÃ´le Emploi | âŒ Optionnel |

---

## ğŸš€ Installation

### PrÃ©requis

- **Python 3.8+** installÃ©
- **pip** (gestionnaire de packages Python)
- **Git** pour clonage du repository
- **ClÃ© API Groq** (gratuite) : https://console.groq.com/

### Ã‰tapes d'Installation

#### 1. Cloner le Repository

```bash
git clone <URL_DU_REPO>
cd CV
```

#### 2. CrÃ©er un Environnement Virtuel

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

#### 3. Installer les DÃ©pendances

```bash
pip install -r requirements.txt
```

**VÃ©rification:**
```bash
pip list | grep -E "Flask|chromadb|sentence-transformers|pandas"
```

#### 4. Configuration des Variables d'Environnement

CrÃ©er le fichier `.env` Ã  la racine:

```bash
# Windows
copy NUL .env

# Linux/Mac
touch .env
```

Ã‰diter `.env` et ajouter:

```env
# OBLIGATOIRE
ATS_API_KEY=gsk_votre_cle_groq_ici

# OPTIONNEL (si scraping Google Jobs)
GOOGLE_JOBS_API_KEY=votre_cle_google

# OPTIONNEL (si scraping France Travail)
FRANCE_TRAVAIL_CLIENT_ID=votre_client_id
FRANCE_TRAVAIL_CLIENT_SECRET=votre_secret

# Flask (gÃ©nÃ©rer une clÃ© alÃ©atoire)
SECRET_KEY=votre_cle_secrete_aleatoire
```

**GÃ©nÃ©rer une SECRET_KEY:**
```python
python -c "import secrets; print(secrets.token_hex(32))"
```

#### 5. Initialiser ChromaDB (IMPORTANT)

**Ã‰tape cruciale** - Ã€ exÃ©cuter une seule fois:

```bash
cd course_scraper
python migrate_embeddings_chromadb.py
```

**Attendu:**
```
Migration terminÃ©e avec succÃ¨s!
- Cours ajoutÃ©s: 16416
- Total dans ChromaDB: 16416
```

**Temps:** ~5-10 minutes

**VÃ©rification:**
```bash
python -c "from course_embedding_store import CourseEmbeddingStore; print(f'Embeddings: {CourseEmbeddingStore().get_count()}')"
```

#### 6. Lancer l'Application

```bash
# Retourner Ã  la racine
cd ..

# Lancer Flask
python app.py
```

**Ou avec le script Windows:**
```bash
START_APP.bat
```

**Sortie attendue:**
```
======================================================================
 PLATEFORME DE MATCHING D'EMPLOIS AVEC ATS
======================================================================
  Application principale: http://localhost:5000
  Dashboard ATS: http://localhost:5000
  Admin Scraping: http://localhost:5000/admin/scraping
  Module de scraping: ACTIVE
======================================================================

âœ… CHROMADB PRÃŠT: 16416/16416 embeddings

 * Running on http://0.0.0.0:5000
```

#### 7. AccÃ©der Ã  l'Application

Ouvrir dans le navigateur:
- **Interface Principale:** http://localhost:5000
- **Statistiques:** http://localhost:5000/stats
- **Admin Scraping:** http://localhost:5000/admin/scraping
- **Admin ChromaDB:** http://localhost:5000/admin/chromadb

---

## âš™ï¸ Configuration

### Configuration des Scrapers

Ã‰diter `job_scraper/config.json`:

```json
{
  "scrapers": {
    "google_jobs": {
      "enabled": true,
      "api_key": "",
      "queries": ["python developer tunisia", "data scientist"],
      "max_results": 200,
      "country": "tn",
      "language": "fr"
    },
    "linkedin": {
      "enabled": true,
      "keywords": ["python", "data science", "devops"],
      "location": "Tunisia",
      "pages": 2,
      "get_descriptions": false
    },
    "france_travail": {
      "enabled": false,
      "client_id": "",
      "client_secret": "",
      "days": 7
    },
    "tunisie_travail": {
      "enabled": true,
      "ville": "tunis",
      "secteur": "informatique",
      "max_pages": 3
    }
  }
}
```

**Ou via l'interface admin:**
1. Aller sur http://localhost:5000/admin/scraping
2. Modifier la configuration
3. Cliquer "Sauvegarder"

### Configuration Flask

Ã‰diter `app.py` (lignes 32-36):

```python
app.config['SECRET_KEY'] = os.getenv('SECRET_KEY', 'changez_moi_en_production')
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max
```

---

## ğŸ”„ Workflows Principaux

### Workflow 1: Recherche d'Emploi avec Analyse ATS

```
1. Utilisateur uploade CV (Mon CV)
   â†“
2. Session stocke CV (user_cvs/<session_id>.pdf)
   â†“
3. Utilisateur recherche offres (/ ou filtres)
   â†“
4. Clic sur une offre (/job/<id>)
   â†“
5. SI CV uploadÃ©:
   a. Extraction texte CV (ats_scorer.extraire_texte_fichier)
   b. Analyse automatique (ats_scorer.analyser_cv_avec_offre)
   c. GÃ©nÃ©ration rapport HTML (score, skills, recommandations)
   â†“
6. Affichage:
   - DÃ©tail de l'offre
   - Score de compatibilitÃ© (0-100%)
   - Skills trouvÃ©es vs manquantes
   - Recommandations de cours Coursera (via ChromaDB)
```

### Workflow 2: Recommandation de Cours Coursera

```
1. Analyse ATS identifie skills manquantes
   â†“
2. Pour chaque skill:
   a. Appel /api/recommend-courses (POST)
   b. ats_scorer.recommander_cours(skill, niveau, contexte)
   c. course_embedding_store.search_similar_courses(query)
      â†’ Recherche vectorielle ChromaDB (< 1 sec)
   d. Scoring hybride:
      - 60% similaritÃ© sÃ©mantique (Sentence-BERT)
      - 40% correspondance nom exact
   e. Filtrage par niveau avec fallback intelligent
   â†“
3. Top 3 cours par skill retournÃ©s
   â†“
4. Affichage avec:
   - Titre, description, organisme
   - DifficultÃ©, durÃ©e
   - Lien Coursera
   - Score de similaritÃ©
```

### Workflow 3: GÃ©nÃ©ration de Tests Techniques

```
1. Utilisateur va sur "Tests Techniques"
   â†“
2. Extraction skills du CV (ats_scorer.extraire_competences_techniques)
   â†“
3. Affichage liste de compÃ©tences avec niveaux
   â†“
4. Clic "GÃ©nÃ©rer Test" pour une skill:
   a. Appel /generate-test/<category>/<index> (POST)
   b. ats_scorer.generer_quiz_coursera(competence, niveau, cv_text)
   c. RÃ©cupÃ©ration modules Coursera (coursera_scraper_utils.scrape_course_modules_details)
   d. GÃ©nÃ©ration 10 QCM via Groq AI basÃ©s sur modules
   e. Distribution: 30% facile, 50% intermÃ©diaire, 20% difficile
   â†“
5. Affichage du test (10 QCM)
   â†“
6. Utilisateur rÃ©pond et soumet
   â†“
7. Ã‰valuation automatique:
   a. Calcul score (0-100%)
   b. DÃ©termination niveau rÃ©el (novice â†’ expert)
   c. Feedback dÃ©taillÃ© par question
   â†“
8. Mise Ã  jour du niveau de compÃ©tence dans session
```

### Workflow 4: VÃ©rification de Documents (Vision AI)

```
1. Utilisateur va sur "VÃ©rifier CompÃ©tences"
   â†“
2. Extraction certificats/attestations du CV
   (ats_scorer.extraire_certificats_attestations via Groq AI)
   â†“
3. Affichage liste (Certifications, Formations, Langues)
   â†“
4. Upload preuve (PDF ou image) pour un Ã©lÃ©ment
   â†“
5. Conversion PDF â†’ Image si nÃ©cessaire (ats_scorer.pdf_to_image)
   â†“
6. VÃ©rification via Vision AI:
   a. Appel /verify-proof/<category>/<index> (POST)
   b. ats_scorer.verifier_document_vlm(claim, proof_path)
   c. Groq Vision API analyse l'image
   d. Comparaison claim CV â†” texte extrait de l'image
   â†“
7. RÃ©sultat:
   - Statut: ConfirmÃ©, Partiellement confirmÃ©, Non confirmÃ©
   - Score de confiance (0-100%)
   - Ã‰lÃ©ments confirmÃ©s vs manquants
   - Commentaire dÃ©taillÃ©
   - Recommandation: Accepter, Demander clarification, Rejeter
```

### Workflow 5: Scraping d'Offres d'Emploi

```
1. Admin va sur /admin/scraping
   â†“
2. Configuration des sources (config.json)
   â†“
3. Clic "Scraper" pour une source (ex: LinkedIn)
   â†“
4. Appel /api/scraping/source/linkedin (POST)
   â†“
5. Thread en arriÃ¨re-plan:
   a. Chargement config (keywords, location, pages)
   b. Scraping via job_scraper/Linkedin.py
   c. Retour liste de jobs (format standardisÃ©)
   d. Insertion via db_manager.bulk_insert_jobs()
      â†’ DÃ©duplication par URL + date
   e. Mise Ã  jour jobs.db
   â†“
6. Suivi temps rÃ©el:
   - Statut: running â†’ completed/error
   - Progression (%)
   - Jobs trouvÃ©s vs doublons
   â†“
7. Rechargement automatique de job_platform.df
   â†“
8. Nouvelles offres visibles sur / instantanÃ©ment
```

---

## ğŸ“¡ API et Endpoints

### Endpoints Publics

#### **GET /** - Page d'accueil
```
ParamÃ¨tres Query:
- keyword: str (recherche titre/description)
- location: str (filtrage localisation)
- company: str (filtrage entreprise)
- job_type: str (CDI, CDD, Stage, etc.)
- contract_type: str (Temps plein, Temps partiel)
- source: str (google_jobs, linkedin, etc.)
- date_range: str (1day, 1week, 1month, 3months, thisyear, all)
- page: int (pagination)

Retour: HTML avec liste d'offres + filtres
```

#### **GET /job/<int:job_id>** - DÃ©tail d'une offre
```
ParamÃ¨tres:
- job_id: Index de l'offre dans jobs.db

Comportement:
- SI CV uploadÃ©: Analyse automatique dÃ¨s chargement
- SINON: Affichage simple de l'offre

Retour: HTML avec dÃ©tail + analyse ATS (si CV)
```

#### **GET /api/search** - Recherche AJAX
```
ParamÃ¨tres Query: (mÃªmes que GET /)

Retour JSON:
{
  "success": true,
  "jobs": [...],
  "stats": {
    "total_jobs": 150,
    "total_pages": 8,
    "current_page": 1,
    "per_page": 20
  }
}
```

#### **POST /api/recommend-courses** - Recommandations Coursera
```
Body JSON:
{
  "missing_skills": ["Python", "Docker", "PostgreSQL"],
  "job_id": 123
}

Retour JSON:
{
  "success": true,
  "recommendations": [
    {
      "title": "Python for Data Science",
      "description": "...",
      "partner": "Stanford University",
      "url": "https://coursera.org/...",
      "difficulty": "INTERMEDIATE",
      "duration": "4 weeks",
      "language": "en",
      "matched_skill": "Python",
      "score_similarite": 87.5
    },
    ...
  ],
  "missing_skills": ["Python", "Docker", "PostgreSQL"],
  "total_skills": 3,
  "total_courses": 9,
  "used_chromadb": true
}
```

### Endpoints CV

#### **POST /upload-cv** - Upload CV
```
Multipart Form:
- cv_file: File (PDF, DOCX, TXT)

Retour JSON:
{
  "success": true,
  "message": "CV uploadÃ© avec succÃ¨s",
  "filename": "mon_cv.pdf"
}

Comportement:
- GÃ©nÃ¨re session_id (UUID) si pas existant
- Sauvegarde: user_cvs/<session_id>.pdf
- Extraction texte pour validation (min 50 chars)
- Stockage en session: cv_uploaded, cv_filename, cv_path
```

#### **POST /remove-cv** - Supprimer CV
```
Retour JSON:
{
  "success": true,
  "message": "CV supprimÃ© avec succÃ¨s"
}

Comportement:
- Supprime fichier physique
- Clear session (cv_uploaded, cv_filename, cv_path, cv_text)
```

#### **GET /check-cv** - VÃ©rifier prÃ©sence CV
```
Retour JSON:
{
  "cv_uploaded": true,
  "cv_filename": "mon_cv.pdf"
}
```

### Endpoints Tests Techniques

#### **GET /technical-tests** - Page tests
```
Comportement:
- Extraction compÃ©tences du CV via Groq AI
- Sauvegarde en session: technical_skills
- Affichage liste compÃ©tences avec boutons "GÃ©nÃ©rer Test"

Retour: HTML
```

#### **POST /generate-test/<category>/<int:skill_index>**
```
ParamÃ¨tres:
- category: "competences_techniques" ou autre
- skill_index: Index dans la liste de compÃ©tences

Body JSON (optionnel):
{
  "difficulte": "moyen|facile|difficile"
}

Retour JSON:
{
  "success": true,
  "test": {
    "competence_testee": "Python",
    "niveau_difficulte": "intermÃ©diaire",
    "questions": [
      {
        "numero": 1,
        "question": "...",
        "options": ["A", "B", "C", "D"],
        "reponse_correcte": "2",
        "explication": "...",
        "difficulte": "facile"
      },
      ...
    ],
    "bareme": {
      "total_points": 100,
      "seuil_reussite": 80,
      "excellent": 90
    }
  }
}

Comportement:
- Recommande cours Coursera pour la compÃ©tence
- Scrape modules du meilleur cours
- GÃ©nÃ¨re 10 QCM basÃ©s sur les modules via Groq AI
- Sauvegarde test en session: generated_tests[test_key]
```

#### **POST /submit-test/<category>/<int:skill_index>**
```
Body JSON:
{
  "reponses": {
    "q_0": "2",
    "q_1": "0",
    ...
  }
}

Retour JSON:
{
  "success": true,
  "evaluation": {
    "score_total": 85,
    "max_points": 100,
    "pourcentage": 85.0,
    "niveau_reel": "avancÃ©",
    "statut": "reussi",
    "message": "CompÃ©tence validÃ©e avec succÃ¨s!",
    "resultats_detailles": [...]
  }
}

Comportement:
- Ã‰value chaque rÃ©ponse (correcte/incorrecte)
- Calcule score total et %
- DÃ©termine niveau rÃ©el basÃ© sur score:
  - 90-100%: expert
  - 75-89%: avancÃ©
  - 60-74%: intermÃ©diaire
  - 40-59%: dÃ©butant
  - 0-39%: novice
- Met Ã  jour technical_skills en session
- Sauvegarde rÃ©sultat: test_results[test_key]
```

### Endpoints VÃ©rification Documents

#### **GET /verify-cv** - Page vÃ©rification
```
Comportement:
- Extraction certificats/attestations du CV via Groq AI
- Sauvegarde en session: credentials_extracted
- Affichage liste avec boutons "Upload Preuve"

Retour: HTML
```

#### **POST /upload-proof/<category>/<int:item_index>**
```
ParamÃ¨tres:
- category: "certifications", "formations", "langues"
- item_index: Index dans la liste

Multipart Form:
- proof_file: File (PDF, JPG, PNG)

Retour JSON:
{
  "success": true,
  "message": "Preuve uploadÃ©e avec succÃ¨s",
  "filename": "0_certificat.pdf",
  "has_preview": true
}

Comportement:
- Sauvegarde: user_proofs/<session_id>/<category>/<item_index>_filename.pdf
- Si PDF: Conversion â†’ image preview (Pillow)
- Stockage en session: uploaded_proofs[proof_key]
```

#### **POST /verify-proof/<category>/<int:item_index>**
```
Retour JSON:
{
  "success": true,
  "verification": {
    "statut": "confirmÃ©|partiellement_confirmÃ©|non_confirmÃ©|insuffisant",
    "score_confiance": 95,
    "elements_confirmes": ["Nom correct", "Organisme validÃ©"],
    "elements_manquants": [],
    "divergences": [],
    "commentaire": "Le certificat confirme l'obtention de...",
    "recommandation": "accepter|demander_clarification|rejeter"
  }
}

Comportement:
- RÃ©cupÃ¨re claim original du CV
- RÃ©cupÃ¨re preuve uploadÃ©e
- Appel ats_scorer.verifier_document_vlm(claim, proof_path, category)
  â†’ Groq Vision API analyse l'image
  â†’ Compare claim vs texte extrait
- Sauvegarde rÃ©sultat: verification_results[proof_key]
```

### Endpoints Admin Scraping

#### **GET /admin/scraping** - Dashboard
```
Retour: HTML avec:
- Statistiques (total jobs, par source, par entreprise)
- Configuration des scrapers
- Boutons lancement par source
- Logs en temps rÃ©el
```

#### **POST /api/scraping/source/<source_name>**
```
ParamÃ¨tres:
- source_name: google_jobs|linkedin|france_travail|tunisie_travail

Retour JSON:
{
  "success": true,
  "message": "Scraping de linkedin dÃ©marrÃ©"
}

Comportement:
- Lance scraping en thread daemon
- Met Ã  jour scraping_status global
- Insertion bulk dans jobs.db via db_manager
```

#### **GET /api/scraping/status**
```
Retour JSON:
{
  "is_running": true,
  "current_source": "linkedin",
  "sources_status": {
    "linkedin": {
      "status": "running",
      "jobs_found": 45,
      "message": "Keyword 2/3: data science",
      "progress": 66
    },
    ...
  },
  "total_jobs": 45,
  "start_time": "2025-11-13T14:30:00",
  "end_time": null
}
```

#### **GET /api/scraping/stats**
```
Retour JSON:
{
  "total_jobs": 5234,
  "by_source": {
    "linkedin": 2100,
    "google_jobs": 1800,
    "tunisie_travail": 1334
  },
  "by_company": {
    "Microsoft": 45,
    "Google": 38,
    ...
  },
  "date_range": {
    "oldest": "2025-01-01",
    "newest": "2025-11-13"
  }
}
```

### Endpoints Admin ChromaDB

#### **GET /admin/chromadb** - Dashboard
```
Retour: HTML avec:
- Statut: Embeddings count vs Total courses
- Migration needed? (boolean)
- Bouton "Migrer" si nÃ©cessaire
- DerniÃ¨re vÃ©rification
```

#### **GET /api/chromadb/status**
```
Retour JSON:
{
  "initialized": true,
  "embeddings_count": 16416,
  "total_courses": 16416,
  "migration_needed": false,
  "migration_running": false,
  "migration_progress": 0,
  "last_check": "2025-11-13T14:00:00",
  "error": null
}
```

#### **POST /api/chromadb/migrate**
```
Retour JSON:
{
  "success": true,
  "message": "Migration dÃ©marrÃ©e en arriÃ¨re-plan"
}

Comportement:
- Lance migration en thread daemon
- Charge tous les cours depuis coursera_fast.db
- GÃ©nÃ¨re embeddings Sentence-BERT
- Stocke dans chroma_db/
- Met Ã  jour chromadb_status global avec progression
```

---

## ğŸ—„ï¸ Base de DonnÃ©es

### jobs.db (SQLite)

#### Table: jobs

```sql
CREATE TABLE jobs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT NOT NULL,
    company TEXT,
    location TEXT,
    description TEXT,
    job_url TEXT UNIQUE,
    date TEXT,
    job_type TEXT,
    salary TEXT,
    contrat TEXT,
    source TEXT,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_job_url ON jobs(job_url);
CREATE INDEX idx_source ON jobs(source);
CREATE INDEX idx_date ON jobs(date);
CREATE INDEX idx_location ON jobs(location);
```

**Colonnes:**
- `id`: ClÃ© primaire auto-incrÃ©mentÃ©e
- `title`: Titre de l'offre (ex: "Data Scientist Senior")
- `company`: Nom de l'entreprise (ex: "Microsoft")
- `location`: Localisation (ex: "Tunis, Tunisia")
- `description`: Description complÃ¨te du poste (HTML ou texte brut)
- `job_url`: URL unique de l'offre (contrainte UNIQUE pour dÃ©duplication)
- `date`: Date de publication (format: "DD/MM/YYYY" ou "YYYY-MM-DD")
- `job_type`: Type de poste (ex: "Temps plein", "Temps partiel")
- `salary`: Salaire (ex: "50000 EUR/an", optionnel)
- `contrat`: Type de contrat (ex: "CDI", "CDD", "Stage")
- `source`: Source du scraping (ex: "linkedin", "google_jobs")
- `created_at`: Date d'insertion en DB

**Volume:** ~5000+ offres (15.6 MB)

**DÃ©duplication:** Par `job_url` (insertion ignorÃ©e si doublon)

---

### course_scraper/coursera_fast.db (SQLite)

#### Table: courses

```sql
CREATE TABLE courses (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    course_id TEXT UNIQUE,
    slug TEXT,
    title TEXT NOT NULL,
    description TEXT,
    partner_name TEXT,
    url TEXT,
    categories TEXT,
    difficulty TEXT,
    duration TEXT,
    language TEXT
);

CREATE INDEX idx_course_id ON courses(course_id);
CREATE INDEX idx_difficulty ON courses(difficulty);
CREATE INDEX idx_title ON courses(title);
```

**Colonnes:**
- `id`: ClÃ© primaire
- `course_id`: ID unique Coursera (ex: "python-fundamentals")
- `slug`: Slug URL (ex: "learn/python-fundamentals")
- `title`: Titre du cours (ex: "Python for Data Science")
- `description`: Description complÃ¨te du cours
- `partner_name`: Organisme (ex: "Stanford University", "Google")
- `url`: URL complÃ¨te Coursera
- `categories`: CatÃ©gories (format JSON array stringifiÃ©)
- `difficulty`: Niveau (BEGINNER, INTERMEDIATE, ADVANCED)
- `duration`: DurÃ©e estimÃ©e (ex: "4 weeks", "20 hours")
- `language`: Langue (ex: "en", "fr")

**Volume:** 16,416 cours (31 MB)

**Mise Ã  jour:** Hebdomadaire via `weekly_update.py`

---

### course_scraper/chroma_db/ (ChromaDB)

#### Collection: coursera_courses

**Structure:**
```python
{
  "ids": ["course_id_1", "course_id_2", ...],
  "embeddings": [
    [0.123, -0.456, ...],  # 384 dimensions (Sentence-BERT)
    [0.789, 0.012, ...],
    ...
  ],
  "metadatas": [
    {
      "title": "Python for Data Science",
      "difficulty": "INTERMEDIATE",
      "duration": "4 weeks",
      "partner_name": "Stanford University",
      "url": "https://coursera.org/..."
    },
    ...
  ],
  "documents": [
    "Python for Data Science - Learn Python programming...",
    ...
  ]
}
```

**ModÃ¨le d'Embeddings:** Sentence-BERT (`all-MiniLM-L6-v2`)
- Dimensions: 384
- Type: Dense vector
- MÃ©trique: Cosinus similarity

**Index:** HNSW (Hierarchical Navigable Small World)
- ComplexitÃ©: O(log N) pour recherche
- Performance: < 1 sec pour 16,416 vecteurs

**Persistance:** Automatique dans `chroma_db/chroma.sqlite3`

---

## ğŸš€ DÃ©ploiement

### DÃ©ploiement sur VPS (Ubuntu/Debian)

#### 1. PrÃ©parer le Serveur

```bash
# Connexion SSH
ssh user@votre-serveur.com

# Mettre Ã  jour le systÃ¨me
sudo apt update && sudo apt upgrade -y

# Installer Python et dÃ©pendances
sudo apt install python3 python3-pip python3-venv nginx -y
```

#### 2. Cloner et Configurer le Projet

```bash
# Cloner le repository
git clone <URL_REPO> /var/www/jobmatch
cd /var/www/jobmatch

# CrÃ©er environnement virtuel
python3 -m venv venv
source venv/bin/activate

# Installer dÃ©pendances
pip install -r requirements.txt
pip install gunicorn  # Serveur WSGI production
```

#### 3. Configuration .env

```bash
# CrÃ©er .env avec vos vraies clÃ©s
nano .env
```

```env
ATS_API_KEY=gsk_VOTRE_CLE_GROQ_PRODUCTION
SECRET_KEY=GENERER_UNE_CLE_ALEATOIRE_LONGUE
FLASK_ENV=production
FLASK_DEBUG=False
```

#### 4. Initialiser ChromaDB

```bash
cd course_scraper
python migrate_embeddings_chromadb.py
cd ..
```

#### 5. Configuration Gunicorn

CrÃ©er `/var/www/jobmatch/gunicorn_config.py`:

```python
bind = "127.0.0.1:8000"
workers = 4  # 2 Ã— CPU cores
worker_class = "sync"
timeout = 120
keepalive = 5
errorlog = "/var/log/jobmatch/error.log"
accesslog = "/var/log/jobmatch/access.log"
loglevel = "info"
```

CrÃ©er dossier logs:
```bash
sudo mkdir -p /var/log/jobmatch
sudo chown -R www-data:www-data /var/log/jobmatch
```

#### 6. Service Systemd

CrÃ©er `/etc/systemd/system/jobmatch.service`:

```ini
[Unit]
Description=JobMatch Pro - Plateforme de Matching d'Emplois
After=network.target

[Service]
Type=notify
User=www-data
Group=www-data
WorkingDirectory=/var/www/jobmatch
Environment="PATH=/var/www/jobmatch/venv/bin"
ExecStart=/var/www/jobmatch/venv/bin/gunicorn -c /var/www/jobmatch/gunicorn_config.py app:app
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

Activer et dÃ©marrer:
```bash
sudo systemctl daemon-reload
sudo systemctl enable jobmatch
sudo systemctl start jobmatch
sudo systemctl status jobmatch
```

#### 7. Configuration Nginx

CrÃ©er `/etc/nginx/sites-available/jobmatch`:

```nginx
server {
    listen 80;
    server_name jobmatch.votredomaine.com;

    # Logs
    access_log /var/log/nginx/jobmatch_access.log;
    error_log /var/log/nginx/jobmatch_error.log;

    # Limite upload (CVs, preuves)
    client_max_body_size 50M;

    # Proxy vers Gunicorn
    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_redirect off;

        # Timeouts pour analyse ATS (peut Ãªtre long)
        proxy_connect_timeout 120s;
        proxy_send_timeout 120s;
        proxy_read_timeout 120s;
    }

    # Fichiers statiques (CSS, JS)
    location /static {
        alias /var/www/jobmatch/static;
        expires 30d;
        add_header Cache-Control "public, immutable";
    }

    # Fichiers uploadÃ©s (CVs, preuves)
    location /user_cvs {
        internal;
        alias /var/www/jobmatch/user_cvs;
    }
}
```

Activer le site:
```bash
sudo ln -s /etc/nginx/sites-available/jobmatch /etc/nginx/sites-enabled/
sudo nginx -t  # VÃ©rifier config
sudo systemctl reload nginx
```

#### 8. SSL avec Let's Encrypt

```bash
# Installer Certbot
sudo apt install certbot python3-certbot-nginx -y

# GÃ©nÃ©rer certificat SSL
sudo certbot --nginx -d jobmatch.votredomaine.com

# Renouvellement automatique (cron)
sudo certbot renew --dry-run
```

#### 9. Permissions

```bash
# DÃ©finir propriÃ©taire
sudo chown -R www-data:www-data /var/www/jobmatch

# Permissions
sudo chmod -R 755 /var/www/jobmatch
sudo chmod -R 770 /var/www/jobmatch/user_cvs
sudo chmod -R 770 /var/www/jobmatch/user_proofs
```

#### 10. VÃ©rification

```bash
# Logs Gunicorn
tail -f /var/log/jobmatch/error.log

# Logs Nginx
tail -f /var/log/nginx/jobmatch_error.log

# Status service
sudo systemctl status jobmatch
```

Tester: https://jobmatch.votredomaine.com

---

### DÃ©ploiement Docker (Optionnel)

#### Dockerfile

CrÃ©er `Dockerfile` Ã  la racine:

```dockerfile
FROM python:3.10-slim

# Installer dÃ©pendances systÃ¨me
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Dossier de travail
WORKDIR /app

# Copier fichiers
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

# CrÃ©er dossiers nÃ©cessaires
RUN mkdir -p uploads user_cvs user_proofs

# Exposer port
EXPOSE 5000

# Variables d'environnement
ENV FLASK_APP=app.py
ENV FLASK_ENV=production

# Commande de dÃ©marrage
CMD ["gunicorn", "-b", "0.0.0.0:5000", "-w", "4", "--timeout", "120", "app:app"]
```

#### docker-compose.yml

```yaml
version: '3.8'

services:
  jobmatch:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./user_cvs:/app/user_cvs
      - ./user_proofs:/app/user_proofs
      - ./jobs.db:/app/jobs.db
      - ./course_scraper:/app/course_scraper
    environment:
      - ATS_API_KEY=${ATS_API_KEY}
      - SECRET_KEY=${SECRET_KEY}
    restart: unless-stopped
```

#### Commandes Docker

```bash
# Build
docker-compose build

# DÃ©marrer
docker-compose up -d

# Logs
docker-compose logs -f

# ArrÃªter
docker-compose down
```

---

## ğŸ”§ Maintenance

### Mise Ã  Jour Hebdomadaire des Cours Coursera

#### Automatisation avec Cron (Linux)

CrÃ©er script `/var/www/jobmatch/scripts/weekly_coursera_update.sh`:

```bash
#!/bin/bash
cd /var/www/jobmatch/course_scraper
source ../venv/bin/activate
python weekly_update.py >> /var/log/jobmatch/coursera_update.log 2>&1
```

Rendre exÃ©cutable:
```bash
chmod +x /var/www/jobmatch/scripts/weekly_coursera_update.sh
```

Ajouter Ã  crontab:
```bash
sudo crontab -e
```

Ajouter ligne:
```cron
# Mise Ã  jour Coursera tous les dimanches Ã  2h AM
0 2 * * 0 /var/www/jobmatch/scripts/weekly_coursera_update.sh
```

#### Automatisation avec Task Scheduler (Windows)

1. Ouvrir Task Scheduler
2. CrÃ©er une tÃ¢che de base:
   - **Nom:** Coursera Weekly Update
   - **Trigger:** Hebdomadaire, dimanche 2h AM
   - **Action:** DÃ©marrer un programme
   - **Programme:** `C:\chemin\vers\venv\Scripts\python.exe`
   - **Arguments:** `C:\chemin\vers\CV\course_scraper\weekly_update.py`

---

### Sauvegarde des DonnÃ©es

#### Script de Backup

CrÃ©er `/var/www/jobmatch/scripts/backup.sh`:

```bash
#!/bin/bash
BACKUP_DIR="/var/backups/jobmatch"
DATE=$(date +%Y%m%d_%H%M%S)
APP_DIR="/var/www/jobmatch"

# CrÃ©er dossier backup
mkdir -p $BACKUP_DIR

# Backup jobs.db
cp $APP_DIR/jobs.db $BACKUP_DIR/jobs_${DATE}.db

# Backup coursera_fast.db
cp $APP_DIR/course_scraper/coursera_fast.db $BACKUP_DIR/coursera_${DATE}.db

# Backup ChromaDB
tar -czf $BACKUP_DIR/chroma_db_${DATE}.tar.gz -C $APP_DIR/course_scraper chroma_db

# Backup CVs et preuves utilisateurs
tar -czf $BACKUP_DIR/user_files_${DATE}.tar.gz -C $APP_DIR user_cvs user_proofs

# Supprimer backups > 30 jours
find $BACKUP_DIR -type f -mtime +30 -delete

echo "Backup terminÃ©: $DATE" >> /var/log/jobmatch/backup.log
```

Rendre exÃ©cutable:
```bash
chmod +x /var/www/jobmatch/scripts/backup.sh
```

Automatiser (cron quotidien Ã  3h AM):
```cron
0 3 * * * /var/www/jobmatch/scripts/backup.sh
```

---

### Monitoring et Logs

#### Logs Ã  Surveiller

| Fichier | Contenu |
|---------|---------|
| `/var/log/jobmatch/error.log` | Erreurs Gunicorn/Flask |
| `/var/log/jobmatch/access.log` | RequÃªtes HTTP |
| `/var/log/nginx/jobmatch_error.log` | Erreurs Nginx |
| `/var/log/jobmatch/coursera_update.log` | Mise Ã  jour cours |
| `/var/log/jobmatch/backup.log` | Historique backups |

#### Commandes Utiles

```bash
# Logs en temps rÃ©el
tail -f /var/log/jobmatch/error.log

# 50 derniÃ¨res erreurs
tail -n 50 /var/log/jobmatch/error.log

# Rechercher erreur spÃ©cifique
grep "ERROR" /var/log/jobmatch/error.log

# Taille des logs
du -sh /var/log/jobmatch/*

# Rotation des logs (logrotate)
sudo nano /etc/logrotate.d/jobmatch
```

Contenu `/etc/logrotate.d/jobmatch`:
```
/var/log/jobmatch/*.log {
    daily
    rotate 14
    compress
    delaycompress
    notifempty
    create 0640 www-data www-data
    sharedscripts
    postrotate
        systemctl reload jobmatch > /dev/null 2>&1 || true
    endscript
}
```

---

### Nettoyage des Fichiers Temporaires

#### Script de Nettoyage

CrÃ©er `/var/www/jobmatch/scripts/cleanup.sh`:

```bash
#!/bin/bash
APP_DIR="/var/www/jobmatch"

# Supprimer fichiers uploads/ > 24h
find $APP_DIR/uploads -type f -mtime +1 -delete

# Supprimer CVs orphelins > 30 jours
# (CVs sans session active)
find $APP_DIR/user_cvs -type f -mtime +30 -delete

# Supprimer preuves orphelines > 30 jours
find $APP_DIR/user_proofs -type d -mtime +30 -exec rm -rf {} +

# Nettoyer cache Python
find $APP_DIR -type d -name "__pycache__" -exec rm -rf {} +

echo "Nettoyage terminÃ©: $(date)" >> /var/log/jobmatch/cleanup.log
```

Automatiser (cron quotidien Ã  4h AM):
```cron
0 4 * * * /var/www/jobmatch/scripts/cleanup.sh
```

---

### Surveillance des Performances

#### VÃ©rifier Utilisation Ressources

```bash
# CPU/RAM
htop

# Espace disque
df -h

# Taille base de donnÃ©es
du -sh /var/www/jobmatch/*.db
du -sh /var/www/jobmatch/course_scraper/*.db

# Nombre de CVs stockÃ©s
ls -1 /var/www/jobmatch/user_cvs | wc -l
```

#### Optimisation ChromaDB

Si ChromaDB devient lent:

```bash
cd /var/www/jobmatch/course_scraper
source ../venv/bin/activate

# RÃ©initialiser et reconstruire
python -c "
from course_embedding_store import CourseEmbeddingStore
store = CourseEmbeddingStore()
store.reset()
print('ChromaDB rÃ©initialisÃ©')
"

# Resynchroniser
python migrate_embeddings_chromadb.py
```

---

## ğŸ“ Support et Contact

### Issues et Bugs

Pour signaler un bug ou demander une fonctionnalitÃ©:
1. Ouvrir une issue sur GitHub
2. Fournir:
   - Description du problÃ¨me
   - Steps to reproduce
   - Logs d'erreur
   - Configuration (OS, Python version)

### Documentation ComplÃ©mentaire

- **README.md** : Documentation utilisateur
- **course_scraper/README_CHROMADB.md** : Guide ChromaDB
- **course_scraper/QUICK_START.md** : DÃ©marrage rapide ChromaDB
- **Ce fichier** : Documentation technique complÃ¨te

---

## ğŸ“Š Statistiques du Projet

| MÃ©trique | Valeur |
|----------|--------|
| **Lignes de Code Python** | ~3,500+ |
| **Fichiers Python** | 15 |
| **Templates HTML** | 12 |
| **Fichiers JavaScript** | 2 |
| **Fichiers CSS** | 2 |
| **Sources de Scraping** | 4 (Google Jobs, LinkedIn, PÃ´le Emploi, Tunisie Travail) |
| **API Endpoints** | 25+ |
| **Cours Coursera** | 16,416 |
| **Embeddings ChromaDB** | 16,416 vecteurs 384D |
| **Taille jobs.db** | 15.6 MB |
| **Taille coursera_fast.db** | 31 MB |
| **Gain performance ChromaDB** | **10x plus rapide** (< 1 sec vs 5-10 sec) |

---

## ğŸ¯ Roadmap Futur

### Version 2.1 (Q1 2026)
- [ ] Authentification utilisateur (login/register)
- [ ] Sauvegarde des recherches favorites
- [ ] Notifications email (nouvelles offres matchant profil)
- [ ] Dashboard candidat amÃ©liorÃ© (historique analyses)

### Version 2.2 (Q2 2026)
- [ ] API REST complÃ¨te avec Swagger documentation
- [ ] Nouveaux scrapers (Indeed, Monster, Welcome to the Jungle)
- [ ] Analyse CV multilingue (franÃ§ais, anglais, arabe)
- [ ] Recommandations personnalisÃ©es via machine learning

### Version 3.0 (Q3 2026)
- [ ] Application mobile (React Native / Flutter)
- [ ] Mode multi-tenants (entreprises)
- [ ] Analytics avancÃ©s (Tableau de bord RH)
- [ ] IntÃ©gration calendriers (planification entretiens)
- [ ] SystÃ¨me de chat (candidat â†” recruteur)

---

## ğŸ™ Remerciements

- **Groq AI** : Pour l'API d'analyse ATS et Vision AI
- **Coursera** : Pour les 16,000+ cours accessibles
- **Bootstrap** : Framework CSS
- **Chart.js** : BibliothÃ¨que de graphiques
- **Flask** : Framework web Python
- **ChromaDB** : Vector database performante
- **Sentence-Transformers** : Embeddings sÃ©mantiques de qualitÃ©

---

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

---

**DÃ©veloppÃ© avec â¤ï¸ par l'Ã©quipe JobMatch Pro**

*DerniÃ¨re mise Ã  jour : 2025-11-13*
