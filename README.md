# ğŸ¯ JobMatch Pro - Plateforme de Matching d'Emplois avec ATS

Une plateforme intelligente de recherche d'emploi avec analyse automatique de CV par ATS (Applicant Tracking System), scraping multi-sources, et systÃ¨me de vÃ©rification des compÃ©tences.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Flask](https://img.shields.io/badge/Flask-3.0+-green.svg)
![SQLite](https://img.shields.io/badge/Database-SQLite-orange.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

---

## ğŸ“‹ Table des matiÃ¨res

- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Architecture](#-architecture)
- [PrÃ©requis](#-prÃ©requis)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Utilisation](#-utilisation)
- [API Documentation](#-api-documentation)
- [Structure du Projet](#-structure-du-projet)
- [DÃ©veloppement](#-dÃ©veloppement)
- [DÃ©ploiement](#-dÃ©ploiement)
- [SÃ©curitÃ©](#-sÃ©curitÃ©)
- [Contribution](#-contribution)

---

## âœ¨ FonctionnalitÃ©s

### ğŸ” Recherche et Matching
- **Recherche Multi-critÃ¨res** : Recherche par mot-clÃ©, localisation, type de contrat, entreprise
- **Filtres AvancÃ©s** : Filtrage par date, source, type d'emploi
- **Pagination** : Navigation optimisÃ©e Ã  travers les rÃ©sultats

### ğŸ¤– Analyse ATS Intelligente
- **Analyse CV vs Offre** : Scoring automatique de compatibilitÃ© (0-100%)
- **Vision Language Model (VLM)** : VÃ©rification automatique des certificats et attestations
- **Extraction Intelligente** : Extraction automatique des compÃ©tences techniques
- **Tests Techniques PersonnalisÃ©s** : GÃ©nÃ©ration de tests adaptÃ©s au profil

### ğŸŒ Web Scraping Multi-sources
- **Google Jobs API** : Scraping via API officielle
- **LinkedIn** : Extraction d'offres sans API (mÃ©thode libre)
- **France Travail** : API officielle PÃ´le Emploi
- **Tunisie Travail** : Scraping du site tunisien

### ğŸ“Š Statistiques et Visualisation
- **Dashboard Temps RÃ©el** : Visualisation des tendances du marchÃ©
- **Graphiques Interactifs** : Chart.js pour les statistiques
- **Export de DonnÃ©es** : Export CSV des rÃ©sultats

### ğŸ” SÃ©curitÃ© et VÃ©rification
- **VÃ©rification de Documents** : Validation automatique des preuves (PDF/Images)
- **Vision API** : Analyse d'images de certificats
- **Gestion SÃ©curisÃ©e** : Variables d'environnement pour les secrets

---

## ğŸ—ï¸ Architecture

```
JobMatch Pro
â”œâ”€â”€ Frontend (Bootstrap 5 + Chart.js)
â”‚   â”œâ”€â”€ Templates Jinja2
â”‚   â””â”€â”€ Interface Responsive
â”‚
â”œâ”€â”€ Backend (Flask)
â”‚   â”œâ”€â”€ Routes REST API
â”‚   â”œâ”€â”€ Session Management
â”‚   â””â”€â”€ File Upload Handling
â”‚
â”œâ”€â”€ Base de DonnÃ©es (SQLite)
â”‚   â”œâ”€â”€ jobs.db (Offres d'emploi)
â”‚   â””â”€â”€ Indexes optimisÃ©s
â”‚
â”œâ”€â”€ Services
â”‚   â”œâ”€â”€ ATS Scorer (Groq AI)
â”‚   â”œâ”€â”€ Job Scrapers
â”‚   â””â”€â”€ VLM Document Verification
â”‚
â””â”€â”€ Storage
    â”œâ”€â”€ user_cvs/ (CVs uploadÃ©s)
    â”œâ”€â”€ user_proofs/ (Preuves/Certificats)
    â””â”€â”€ uploads/ (Fichiers temporaires)
```

---

## ğŸ“¦ PrÃ©requis

### Logiciels Requis
- **Python** : 3.8 ou supÃ©rieur
- **pip** : Gestionnaire de paquets Python
- **Git** : Pour le versioning

### Comptes et API Keys
- **Groq API** : ClÃ© API pour l'analyse ATS (gratuit)
  - Inscription : https://console.groq.com/
- **Google Jobs API** *(optionnel)* : Pour le scraping Google Jobs
- **France Travail API** *(optionnel)* : Client ID + Secret

---

## ğŸš€ Installation

### 1. Cloner le Repository

```bash
git clone https://github.com/MChtara/recruScore.git
cd recruScore
```

### 2. CrÃ©er un Environnement Virtuel

**Windows :**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac :**
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Installer les DÃ©pendances

```bash
pip install -r requirements.txt
```

**DÃ©pendances Principales :**
- `flask` : Framework web
- `pandas` : Manipulation de donnÃ©es
- `requests` : RequÃªtes HTTP
- `python-dotenv` : Gestion des variables d'environnement
- `PyPDF2` ou `pdfplumber` : Extraction de texte PDF
- `python-docx` : Lecture de fichiers Word
- `Pillow` : Traitement d'images
- `beautifulsoup4` : Web scraping
- `selenium` *(optionnel)* : Scraping dynamique

---

## âš™ï¸ Configuration

### 1. Fichier d'Environnement

CrÃ©er un fichier `.env` Ã  la racine du projet :

Ã‰diter `.env` et remplir vos clÃ©s :

```env
# Groq API Key (OBLIGATOIRE)
ATS_API_KEY=gsk_votre_cle_api_ici
```

### 2. Configuration des Scrapers

Le fichier `job_scraper/config.json` contient la configuration des scrapers :

```json
{
  "scrapers": {
    "google_jobs": {
      "enabled": true,
      "api_key": "",
      "queries": ["dÃ©veloppeur python tunisie", "data scientist"],
      "max_results": 200,
      "country": "tn",
      "language": "fr"
    },
    "linkedin": {
      "enabled": true,
      "keywords": ["python", "data science"],
      "location": "Tunisia",
      "pages": 2
    }
  }
}
```

### 3. Initialiser la Base de DonnÃ©es

La base de donnÃ©es SQLite sera crÃ©Ã©e automatiquement au premier lancement :

```bash
python app.py
```

---

## ğŸ’» Utilisation

### DÃ©marrer l'Application

```bash
python app.py
```

L'application sera accessible sur :
- **Interface Principale** : http://localhost:5000
- **Statistiques** : http://localhost:5000/stats
- **Admin Scraping** : http://localhost:5000/admin/scraping

### Workflow Utilisateur

1. **Upload de CV** :
   - Aller sur "Mon CV"
   - Uploader un fichier PDF/DOCX

2. **Recherche d'Emplois** :
   - Utiliser la barre de recherche
   - Appliquer des filtres

3. **Analyse ATS** :
   - Cliquer sur une offre
   - L'analyse se fait automatiquement si un CV est uploadÃ©

4. **VÃ©rification de CompÃ©tences** :
   - Aller sur "Tests Techniques"
   - SÃ©lectionner une compÃ©tence
   - Passer le test gÃ©nÃ©rÃ©

### Workflow Admin

1. **Configuration du Scraping** :
   - Aller sur `/admin/scraping`
   - Configurer les sources

2. **Lancer le Scraping** :
   - Cliquer sur "Scraper" pour une source
   - Suivre la progression en temps rÃ©el

3. **Exporter les DonnÃ©es** :
   - Utiliser le bouton "Export CSV"

---

## ğŸ“¡ API Documentation

### Endpoints Principaux

#### Recherche d'Emplois
```http
GET /api/search?keyword=python&location=tunis&page=1
```

**RÃ©ponse :**
```json
{
  "jobs": [...],
  "stats": {
    "total_jobs": 150,
    "total_pages": 8,
    "current_page": 1
  }
}
```

#### Upload de CV
```http
POST /upload-cv
Content-Type: multipart/form-data

cv_file: <file>
```

**RÃ©ponse :**
```json
{
  "success": true,
  "message": "CV uploadÃ© avec succÃ¨s",
  "filename": "mon_cv.pdf"
}
```

#### Analyse ATS
```http
GET /job/<job_id>
```

Retourne l'analyse automatique si un CV est uploadÃ©.

#### Scraping Status
```http
GET /api/scraping/status
```

**RÃ©ponse :**
```json
{
  "is_running": true,
  "current_source": "linkedin",
  "sources_status": {...}
}
```

---

## ğŸ“ Structure du Projet

```
CV/
â”œâ”€â”€ app.py                          # Application Flask principale
â”œâ”€â”€ ats_scorer.py                   # Module d'analyse ATS
â”œâ”€â”€ .env                            # Variables d'environnement (non versionnÃ©)
â”œâ”€â”€ .env.example                    # Template des variables
â”œâ”€â”€ requirements.txt                # DÃ©pendances Python
â”œâ”€â”€ jobs.db                         # Base de donnÃ©es SQLite
â”‚
â”œâ”€â”€ job_scraper/                    # Modules de scraping
â”‚   â”œâ”€â”€ db_manager.py              # Gestionnaire de base de donnÃ©es
â”‚   â”œâ”€â”€ google_jobs.py             # Scraper Google Jobs
â”‚   â”œâ”€â”€ Linkedin.py                # Scraper LinkedIn
â”‚   â”œâ”€â”€ france_travail.py          # API France Travail
â”‚   â”œâ”€â”€ tunisietravail.py          # Scraper Tunisie Travail
â”‚   â””â”€â”€ config.json                # Configuration des scrapers
â”‚
â”œâ”€â”€ templates/                      # Templates HTML (Jinja2)
â”‚   â”œâ”€â”€ base.html                  # Template de base
â”‚   â”œâ”€â”€ index.html                 # Page d'accueil / Recherche
â”‚   â”œâ”€â”€ job_detail.html            # DÃ©tail d'une offre + ATS
â”‚   â”œâ”€â”€ stats.html                 # Dashboard statistiques
â”‚   â”œâ”€â”€ upload_cv_page.html        # Gestion du CV
â”‚   â”œâ”€â”€ verify_credentials.html    # VÃ©rification des certificats
â”‚   â”œâ”€â”€ technical_tests.html       # Tests techniques
â”‚   â””â”€â”€ admin_scraping.html        # Administration du scraping
â”‚
â”œâ”€â”€ static/                         # Fichiers statiques
â”‚   â”œâ”€â”€ css/
â”‚   â””â”€â”€ js/
â”‚       â”œâ”€â”€ main.js
â”‚       â””â”€â”€ app.js
â”‚
â”œâ”€â”€ user_cvs/                       # CVs uploadÃ©s (non versionnÃ©)
â”œâ”€â”€ user_proofs/                    # Preuves/Certificats (non versionnÃ©)
â””â”€â”€ uploads/                        # Fichiers temporaires (non versionnÃ©)
```

---

## ğŸ› ï¸ DÃ©veloppement

### Installation en Mode DÃ©veloppement

```bash
# Activer le mode debug dans .env
FLASK_DEBUG=True

# Lancer avec rechargement automatique
python app.py
```

### Ajouter un Nouveau Scraper

1. CrÃ©er un fichier dans `job_scraper/` :

```python
# job_scraper/nouveau_scraper.py

def scrape_nouveau_site(keyword: str, location: str):
    """Scraper pour un nouveau site"""
    jobs = []

    # Logique de scraping ici
    # ...

    return jobs  # Liste de dictionnaires
```

2. Format de retour :

```python
job = {
    'title': 'Titre du poste',
    'company': 'Nom entreprise',
    'location': 'Ville, Pays',
    'description': 'Description complÃ¨te',
    'job_url': 'https://...',
    'date': '2025-01-15',
    'job_type': 'CDI/CDD/Stage',
    'salary': '50000 EUR/an',
    'contrat': 'CDI'
}
```

3. Ajouter dans `config.json` :

```json
"nouveau_scraper": {
  "enabled": true,
  "param1": "valeur"
}
```

4. IntÃ©grer dans `app.py` :

```python
elif source_name == 'nouveau_scraper':
    from nouveau_scraper import scrape_nouveau_site
    jobs = scrape_nouveau_site(...)
```

### Tests

```bash
# Tester le gestionnaire de base de donnÃ©es
python job_scraper/db_manager.py

# Tester l'ATS Scorer
python ats_scorer.py

# Tester un scraper spÃ©cifique
python job_scraper/linkedin.py
```

---

## ğŸš€ DÃ©ploiement

### DÃ©ploiement sur un VPS (Ubuntu/Debian)

```bash
# 1. Installer les dÃ©pendances systÃ¨me
sudo apt update
sudo apt install python3-pip python3-venv nginx

# 2. Cloner le projet
git clone <repo_url>
cd recruScore

# 3. Configuration
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install gunicorn

# 4. CrÃ©er le fichier .env en production
cp .env.example .env
nano .env  # Ã‰diter avec vos clÃ©s

# 5. Lancer avec Gunicorn
gunicorn -w 4 -b 0.0.0.0:8000 app:app
```

### Configuration Nginx

```nginx
server {
    listen 80;
    server_name votre-domaine.com;

    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    location /static {
        alias /chemin/vers/recruScore/static;
    }
}
```

### Service Systemd

CrÃ©er `/etc/systemd/system/jobmatch.service` :

```ini
[Unit]
Description=JobMatch Pro Application
After=network.target

[Service]
User=www-data
WorkingDirectory=/chemin/vers/recruScore
Environment="PATH=/chemin/vers/recruScore/venv/bin"
ExecStart=/chemin/vers/recruScore/venv/bin/gunicorn -w 4 -b 127.0.0.1:8000 app:app

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl daemon-reload
sudo systemctl start jobmatch
sudo systemctl enable jobmatch
```

---

## ğŸ”’ SÃ©curitÃ©

### Bonnes Pratiques ImplÃ©mentÃ©es

âœ… **Variables d'Environnement** : Toutes les clÃ©s API sont dans `.env`
âœ… **Gitignore** : Fichiers sensibles exclus du versioning
âœ… **Validation des Fichiers** : VÃ©rification des extensions uploadÃ©es
âœ… **Session Management** : Sessions Flask sÃ©curisÃ©es
âœ… **SQL Injection Protection** : RequÃªtes paramÃ©trÃ©es (SQLite)

### Recommandations de Production

âš ï¸ **Changez la SECRET_KEY** dans `.env`
âš ï¸ **DÃ©sactivez FLASK_DEBUG** en production
âš ï¸ **Utilisez HTTPS** (Let's Encrypt)
âš ï¸ **Limitez les uploads** (taille de fichier)
âš ï¸ **Backup rÃ©gulier** de `jobs.db`
âš ï¸ **Rate Limiting** sur les API endpoints

### RÃ©vocation des ClÃ©s API

Si une clÃ© API est compromise :

1. **Groq API** : https://console.groq.com/keys â†’ Supprimer la clÃ©
2. **GÃ©nÃ©rer une nouvelle clÃ©**
3. **Mettre Ã  jour `.env`**
4. **RedÃ©marrer l'application**

---

## ğŸ¤ Contribution

### Workflow Git

```bash
# 1. CrÃ©er une branche feature
git checkout -b feature/nouvelle-fonctionnalite

# 2. Faire vos modifications
git add .
git commit -m "feat: Description de la fonctionnalitÃ©"

# 3. Pousser et crÃ©er une Pull Request
git push origin feature/nouvelle-fonctionnalite
```

### Convention de Commits

- `feat:` Nouvelle fonctionnalitÃ©
- `fix:` Correction de bug
- `docs:` Documentation
- `style:` Formatage (pas de changement de code)
- `refactor:` Refactorisation
- `test:` Ajout de tests
- `chore:` Maintenance

### Code Review Checklist

- [ ] Le code suit les conventions Python (PEP 8)
- [ ] Les secrets ne sont pas commitÃ©es
- [ ] Les fonctions ont des docstrings
- [ ] Les imports sont organisÃ©s
- [ ] Les erreurs sont gÃ©rÃ©es (try/except)
- [ ] Les logs sont informatifs

---

## ğŸ“ License

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ‘¥ Ã‰quipe de DÃ©veloppement

**Maintainers :**
- [@MChtara](https://github.com/MChtara) - Lead Developer

**Contributors :**
- Voir [CONTRIBUTORS.md](CONTRIBUTORS.md)

---

## ğŸ“ Support

**Issues** : [GitHub Issues](https://github.com/MChtara/recruScore/issues)
**Documentation** : [Wiki](https://github.com/MChtara/recruScore/wiki)
**Email** : support@jobmatchpro.com

---

## ğŸ—ºï¸ Roadmap

### Version 1.1 (Q1 2025)
- [ ] Authentification utilisateur (Login/Register)
- [ ] Sauvegarde des recherches favorites
- [ ] Notifications par email
- [ ] Dashboard candidat amÃ©liorÃ©

### Version 1.2 (Q2 2025)
- [ ] API REST complÃ¨te avec documentation Swagger
- [ ] IntÃ©gration de nouveaux scrapers (Indeed, Monster)
- [ ] Analyse de CV par IA amÃ©liorÃ©e
- [ ] Recommandations personnalisÃ©es

### Version 2.0 (Q3 2025)
- [ ] Application mobile (React Native)
- [ ] Mode multi-tenants (entreprises)
- [ ] Analytics avancÃ©s
- [ ] IntÃ©gration avec calendriers (entretiens)

---

## ğŸ“Š Statistiques du Projet

- **Lignes de Code** : ~3,500
- **Modules Python** : 8
- **Templates HTML** : 11
- **Sources de Scraping** : 4
- **API Endpoints** : 15+

---

## ğŸ™ Remerciements

- **Groq AI** : Pour l'API d'analyse ATS
- **Bootstrap** : Framework CSS
- **Chart.js** : BibliothÃ¨que de graphiques
- **Flask** : Framework web Python
- **SQLite** : Base de donnÃ©es lÃ©gÃ¨re

---

**Fait avec â¤ï¸ par l'Ã©quipe JobMatch Pro**
